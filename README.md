# ƒê·ªì √°n m√¥n h·ªçc CS221: AnglE-optimized Embeddings üìê
<p align="center">
  <img src="assets/framework.png" alt="Overall Framework" width="600"/>
</p>

**V·ªÅ chi ti·∫øt c√°ch s·ª≠ d·ª•ng, m·ªçi ng∆∞·ªùi c√≥ th·ªÉ ƒë·ªçc t·∫°i üìò t√†i li·ªáu n√†y:** https://angle.readthedocs.io/en/latest/index.html

üì¢ **Train/Infer Powerful Sentence Embeddings with AnglE.**

C√≥ th·ªÉ s·ª≠ d·ª•ng th∆∞ vi·ªán ƒë·ªÉ √°p d·ª•ng v√†o b√†i to√°n kh√°c 1 c√°ch ti·ªán l·ª£i b·∫±ng c√°ch t·∫£i th√¥ng qua: https://pypi.org/project/angle-emb/ ho·∫∑c l·ªánh pip install angle-emb

Th∆∞ vi·ªán n√†y t·ª´ paper: [AnglE: Angle-optimized Text Embeddings](https://arxiv.org/abs/2309.12871).

## ‚ú® C·∫•u tr√∫c th∆∞ m·ª•c

```
Angle_Embedding/
‚îú‚îÄ‚îÄ .gitignore              # Danh s√°ch file/th∆∞ m·ª•c b·ªã lo·∫°i kh·ªèi git
‚îú‚îÄ‚îÄ .python-version         # Phi√™n b·∫£n Python s·ª≠ d·ª•ng cho d·ª± √°n
‚îú‚îÄ‚îÄ .readthedocs.yaml       # C·∫•u h√¨nh build t√†i li·ªáu tr√™n ReadTheDocs
‚îú‚îÄ‚îÄ angle_emb/              # Th∆∞ vi·ªán ch√≠nh: m√£ ngu·ªìn AnglE (model, trainer, loss, utils)
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py         # Kh·ªüi t·∫°o package Python
‚îÇ   ‚îú‚îÄ‚îÄ angle.py            # ƒê·ªãnh nghƒ©a l·ªõp AnglE v√† c√°c ch·ª©c nƒÉng ch√≠nh
‚îÇ   ‚îú‚îÄ‚îÄ angle_trainer.py    # Module hu·∫•n luy·ªán m√¥ h√¨nh AnglE
‚îÇ   ‚îú‚îÄ‚îÄ base.py             # L·ªõp c∆° s·ªü cho c√°c m√¥ h√¨nh embedding
‚îÇ   ‚îú‚îÄ‚îÄ evaluation.py       # ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng embedding (Spearman, Pearson, ...)
‚îÇ   ‚îú‚îÄ‚îÄ loss.py             # ƒê·ªãnh nghƒ©a c√°c h√†m loss (Angle, Contrastive, Espresso, ...)
‚îÇ   ‚îú‚îÄ‚îÄ utils.py            # C√°c h√†m ti·ªán √≠ch d√πng chung
‚îÇ   ‚îú‚îÄ‚îÄ version.py          # Th√¥ng tin phi√™n b·∫£n th∆∞ vi·ªán
‚îú‚îÄ‚îÄ assets/                 # T√†i nguy√™n b·ªï sung (h√¨nh ·∫£nh, bi·ªÉu ƒë·ªì, ...)
‚îú‚îÄ‚îÄ docs/                   # T√†i li·ªáu d·ª± √°n (Sphinx, h∆∞·ªõng d·∫´n, ghi ch√∫, c·∫•u h√¨nh)
‚îÇ   ‚îú‚îÄ‚îÄ conf.py             # C·∫•u h√¨nh Sphinx
‚îÇ   ‚îú‚îÄ‚îÄ index.rst           # Trang ch·ªß t√†i li·ªáu
‚îÇ   ‚îú‚îÄ‚îÄ Makefile, make.bat  # Script build t√†i li·ªáu
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt    # Y√™u c·∫ßu c√†i ƒë·∫∑t cho t√†i li·ªáu
‚îÇ   ‚îî‚îÄ‚îÄ notes/              # C√°c ghi ch√∫, h∆∞·ªõng d·∫´n chi ti·∫øt
‚îú‚îÄ‚îÄ en_results/             # K·∫øt qu·∫£ ƒë√°nh gi√° m√¥ h√¨nh ti·∫øng Anh (json, b√°o c√°o)
‚îÇ   ‚îî‚îÄ‚îÄ UAE-Large-V1/       # K·∫øt qu·∫£ cho model UAE-Large-V1
‚îú‚îÄ‚îÄ examples/               # V√≠ d·ª• s·ª≠ d·ª•ng, notebook, script hu·∫•n luy·ªán v√† ƒë√°nh gi√°
‚îÇ   ‚îú‚îÄ‚îÄ Angle-ATEC.ipynb    # Notebook v√≠ d·ª• cho b·ªô d·ªØ li·ªáu ATEC
‚îÇ   ‚îú‚îÄ‚îÄ Angle-BQ.ipynb      # Notebook v√≠ d·ª• cho b·ªô d·ªØ li·ªáu BQ
‚îÇ   ‚îú‚îÄ‚îÄ Angle-LCQMC.ipynb   # Notebook v√≠ d·ª• cho b·ªô d·ªØ li·ªáu LCQMC
‚îÇ   ‚îú‚îÄ‚îÄ Angle-PAWSX.ipynb   # Notebook v√≠ d·ª• cho b·ªô d·ªØ li·ªáu PAWSX
‚îÇ   ‚îú‚îÄ‚îÄ multigpu_infer.py   # V√≠ d·ª• inference ƒëa GPU
‚îÇ   ‚îú‚îÄ‚îÄ NLI/                # V√≠ d·ª• v·ªÅ Natural Language Inference
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SentEval/       # B·ªô toolkit ƒë√°nh gi√° embedding (SentEval)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README.md   # H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng SentEval
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ setup.py    # C√†i ƒë·∫∑t SentEval
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ senteval/   # M√£ ngu·ªìn c√°c task ƒë√°nh gi√° (STS, SICK, probing, ...)
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ probing.py
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ sick.py
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ sts.py
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ engine.py
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ tools/
‚îÇ   ‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ ranking.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ eval_nli.py     # Script ƒë√°nh gi√° NLI
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ eval_ese_nli.py # Script ƒë√°nh gi√° ESE NLI
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_nli.py    # Script hu·∫•n luy·ªán NLI
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data/           # Script t·∫£i d·ªØ li·ªáu NLI
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ download_data.sh
‚îÇ   ‚îî‚îÄ‚îÄ UAE/                # V√≠ d·ª• v·ªÅ Universal AnglE Embeddings
‚îÇ       ‚îú‚îÄ‚îÄ README.md
‚îÇ       ‚îú‚îÄ‚îÄ compute_scores.py # t√≠nh ƒëi·ªÉm
‚îÇ       ‚îú‚îÄ‚îÄ emb_model.py
‚îÇ       ‚îú‚îÄ‚îÄ run_eval_mteb.py # ƒê√°nh gi√° tr√™n MTEB
‚îÇ       ‚îî‚îÄ‚îÄ train.py # Hu·∫•n luy·ªán m√¥ h√¨nh
‚îú‚îÄ‚îÄ LICENSE                 # Gi·∫•y ph√©p s·ª≠ d·ª•ng m√£ ngu·ªìn (MIT)
‚îú‚îÄ‚îÄ MIGRATION_GUIDE.md      # H∆∞·ªõng d·∫´n n√¢ng c·∫•p phi√™n b·∫£n m·ªõi nh·∫•t
‚îú‚îÄ‚îÄ pyproject.toml          # C·∫•u h√¨nh build v√† metadata d·ª± √°n Python
‚îú‚îÄ‚îÄ README.md               # Gi·ªõi thi·ªáu v·ªÅ ƒë·ªì √°n
‚îú‚îÄ‚îÄ README_2DMSE.md         # T√†i li·ªáu v·ªÅ 2D Matryoshka Sentence Embeddings
‚îú‚îÄ‚îÄ README_ESE.md           # T√†i li·ªáu v·ªÅ Espresso Sentence Embeddings
‚îú‚îÄ‚îÄ README_zh.md            # T√†i li·ªáu ti·∫øng Trung
‚îú‚îÄ‚îÄ requirements.txt        # Y√™u c·∫ßu c√†i ƒë·∫∑t Python cho d·ª± √°n
‚îú‚îÄ‚îÄ ruff.toml               # C·∫•u h√¨nh linting v·ªõi ruff
‚îú‚îÄ‚îÄ scripts/                # Script ti·ªán √≠ch, chuy·ªÉn ƒë·ªïi m√¥ h√¨nh, x·ª≠ l√Ω d·ªØ li·ªáu
‚îÇ   ‚îî‚îÄ‚îÄ convert_to_sentence_transformer.py
‚îú‚îÄ‚îÄ tests/                  # test th·ª≠ m√¥ h√¨nh nhanh
```


**Backbones**:
- BERT-based models (BERT, RoBERTa, ModernBERT, etc.)
- LLM-based models (LLaMA, Mistral, Qwen, etc.)
- Bi-directional LLM-based models (LLaMA, Mistral, Qwen, OpenELMo, etc.. refer to: https://github.com/WhereIsAI/BiLLM)

**Training**:
- Single-GPU training
- Multi-GPU training



## üõ†Ô∏è C√†i ƒë·∫∑t
### S·ª≠ d·ª•ng Conda

```bash
git clone https://github.com/BPhucKHMT/Angle_Embedding.git
cd Angle_Embedding

# T·∫°o environment m·ªõi v·ªõi Python 3.10
conda create -n angle python=3.10 -y

# K√≠ch ho·∫°t environment
conda activate angle

pip install -e .
```


## üöÄ Th·ª±c nghi·ªám 

### STS Benchmark
#### S·ª≠ d·ª•ng pretrain model
S·ª≠ d·ª•ng c√°c model ƒë√£ pretrain d∆∞·ªõi ƒë√¢y ƒë·ªÉ ƒë√°nh gi√° nhanh
##### ü§ó HF Pretrained Models

[AnglE NLI Sentence Embedding](https://huggingface.co/collections/SeanLee97/angle-nli-sentence-embeddings-6646de386099d0472c5e21c0)

##### English STS Results

| Model | STS12 | STS13 | STS14 | STS15 | STS16 | STSBenchmark | SICKRelatedness |  Avg. |
| ------- |-------|-------|-------|-------|-------|--------------|-----------------|-------|
| [SeanLee97/angle-llama-7b-nli-20231027](https://huggingface.co/SeanLee97/angle-llama-7b-nli-20231027) | 78.68 | 90.58 | 85.49 | 89.56 | 86.91 |    88.92     |      81.18      | 85.90 |
| [SeanLee97/angle-llama-7b-nli-v2](https://huggingface.co/SeanLee97/angle-llama-7b-nli-v2) | 79.00 | 90.56 | 85.79 | 89.43 | 87.00 |    88.97     |      80.94      | 85.96 |
| [SeanLee97/angle-llama-13b-nli](https://huggingface.co/SeanLee97/angle-llama-13b-nli)  | 79.33 | 90.65 | 86.89 | 90.45 | 87.32 |    89.69     |      81.32       | **86.52** |
| [SeanLee97/angle-bert-base-uncased-nli-en-v1](https://huggingface.co/SeanLee97/angle-bert-base-uncased-nli-en-v1) | 75.09 | 85.56 | 80.66 | 86.44 | 82.47 | 85.16 | 81.23 | 82.37 |
---

**BERT**

```bash
python eval_nli.py \
--model_name_or_path SeanLee97/angle-bert-base-uncased-nli-en-v1 \
--pooling_strategy cls_avg
```
**LLM-based**

```bash
python eval_nli.py \
--model_name_or_path SeanLee97/angle-llama-7b-nli-v2 \
--pooling_strategy cls_avg
```

---


## üï∏Ô∏è Custom Training

> üí° For complete details, see the [official training documentation](https://angle.readthedocs.io/en/latest/notes/training.html).

---

### üóÇÔ∏è Step 1: Prepare Your Dataset

AnglE supports three dataset formats. Choose based on your task:

| Format | Columns | Description | Use Case |
|--------|---------|-------------|----------|
| **Format A** | `text1`, `text2`, `label` | Paired texts with similarity scores (0-1) | Similarity scoring |
| **Format B** | `query`, `positive` | Query-document pairs | Retrieval without hard negatives |
| **Format C** | `query`, `positive`, `negative` | Query with positive and negative samples | Contrastive learning |

**Notes:**
- All formats use HuggingFace `datasets.Dataset`
- `text1`, `text2`, `query`, `positive`, and `negative` can be `str` or `List[str]` (random sampling for lists)

---

### üöÇ Step 2: Training Methods

#### Option A: CLI Training (Recommended)

**Single GPU:**

```bash
CUDA_VISIBLE_DEVICES=0 angle-trainer --help
```

**Multi-GPU with FSDP:**

```bash
CUDA_VISIBLE_DEVICES=0,1,2,3 WANDB_MODE=disabled accelerate launch \
  --multi_gpu \
  --num_processes 4 \
  --main_process_port 2345 \
  --config_file examples/FSDP/fsdp_config.yaml \
  -m angle_emb.angle_trainer \
  --gradient_checkpointing 1 \
  --use_reentrant 0 \
  ...
```

**Multi-GPU (Standard):**

```bash
CUDA_VISIBLE_DEVICES=0,1,2,3 WANDB_MODE=disabled accelerate launch \
  --multi_gpu \
  --num_processes 4 \
  --main_process_port 2345 \
  -m angle_emb.angle_trainer \
  --model_name_or_path YOUR_MODEL \
  --train_name_or_path YOUR_DATASET \
  ...
```

üìÅ More examples: [examples/Training](examples/Training)

---

#### Option B: Python API Training
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1h28jHvv_x-0fZ0tItIMjf8rJGp3GcO5V?usp=sharing)

```python
from datasets import load_dataset
from angle_emb import AnglE

# Step 1: Load pretrained model
angle = AnglE.from_pretrained(
    'SeanLee97/angle-bert-base-uncased-nli-en-v1',
    max_length=128,
    pooling_strategy='cls'
).cuda()

# Step 2: Prepare dataset (Format A example)
ds = load_dataset('mteb/stsbenchmark-sts')
ds = ds.map(lambda obj: {
    "text1": str(obj["sentence1"]),
    "text2": str(obj['sentence2']),
    "label": obj['score']
})
ds = ds.select_columns(["text1", "text2", "label"])

# Step 3: Train the model
angle.fit(
    train_ds=ds['train'].shuffle(),
    valid_ds=ds['validation'],
    output_dir='ckpts/sts-b',
    batch_size=32,
    epochs=5,
    learning_rate=2e-5,
    save_steps=100,
    eval_steps=1000,
    warmup_steps=0,
    gradient_accumulation_steps=1,
    loss_kwargs={
        'cosine_w': 1.0,
        'ibn_w': 1.0,
        'angle_w': 0.02,
        'cosine_tau': 20,
        'ibn_tau': 20,
        'angle_tau': 20
    },
    fp16=True,
    logging_steps=100
)

# Step 4: Evaluate
corrcoef = angle.evaluate(ds['test'])
print('Spearman\'s corrcoef:', corrcoef)
```

---

### ‚öôÔ∏è Advanced Configuration

#### Training Special Models

| Model Type | CLI Flags | Description |
|------------|-----------|-------------|
| **LLM** | `--is_llm 1` + LoRA params | Must manually enable LLM mode |
| **BiLLM** | `--apply_billm 1 --billm_model_class LlamaForCausalLM` | Bidirectional LLMs ([guide](https://github.com/WhereIsAI/BiLLM)) |
| **Espresso (ESE)** | `--apply_ese 1 --ese_kl_temperature 1.0 --ese_compression_size 256` | Matryoshka-style embeddings |

#### Applying Prompts

| Format | Flag | Applies To |
|--------|------|------------|
| Format A | `--text_prompt "text: {text}"` | Both `text1` and `text2` |
| Format B/C | `--query_prompt "query: {text}"` | `query` field |
| Format B/C | `--doc_prompt "document: {text}"` | `positive` and `negative` fields |

#### Column Mapping (Legacy Compatibility)

Adapt old datasets without modification:

```bash
# CLI
--column_rename_mapping "text:query"

# Python
column_rename_mapping={"text": "query"}
```

#### Model Conversion

Convert trained models to `sentence-transformers` format:

```bash
python scripts/convert_to_sentence_transformers.py --help
```

---

### üí° Fine-tuning Tips

üìñ [Full documentation](https://angle.readthedocs.io/en/latest/notes/training.html#fine-tuning-tips)

| Format | Recommendation |
|--------|----------------|
| **Format A** | Increase `cosine_w` or decrease `ibn_w` |
| **Format B** | Only tune `ibn_w` and `ibn_tau` |
| **Format C** | Set `cosine_w=0`, `angle_w=0.02`, and configure `cln_w` + `ibn_w` |

**Prevent Catastrophic Forgetting:**
- Set `teacher_name_or_path` for knowledge distillation
- Use same model path for self-distillation
- ‚ö†Ô∏è Ensure teacher and student use the **same tokenizer**

---

### üîÑ Integration with sentence-transformers

| Task | Status | Notes |
|------|--------|-------|
| **Training** | ‚ö†Ô∏è Partial | SentenceTransformers has [AnglE loss](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#angleloss), but use official `angle_emb` for best results |
| **Inference** | ‚úÖ Full | Convert trained models: `examples/convert_to_sentence_transformers.py` |


# ü´° Citation

If you use our code and pre-trained models, please support us by citing our work as follows:

```bibtex
@article{li2023angle,
  title={AnglE-optimized Text Embeddings},
  author={Li, Xianming and Li, Jing},
  journal={arXiv preprint arXiv:2309.12871},
  year={2023}
}
```

# üìú ChangeLogs

| üìÖ | Description |
|----|------|
| 2025 Jan |  **v0.6.0 - Major refactoring** üéâ: <br/>‚Ä¢ Removed `AngleDataTokenizer` - no need to pre-tokenize datasets!<br/>‚Ä¢ Removed `DatasetFormats` class - use string literals ('A', 'B', 'C')<br/>‚Ä¢ Removed auto-detection of LLM models - set `is_llm` manually<br/>‚Ä¢ Renamed `--prompt_template` to `--text_prompt` (Format A only)<br/>‚Ä¢ Added `--query_prompt` and `--doc_prompt` for Format B/C<br/>‚Ä¢ Added `--column_rename_mapping` to adapt old datasets without modification<br/>‚Ä¢ Updated data formats: Format B/C now use `query`, `positive`, `negative` fields<br/>‚Ä¢ Support list-based sampling in Format B/C<br/>‚Ä¢ Updated examples to use `accelerate launch`<br/>‚Ä¢ See [MIGRATION_GUIDE.md](MIGRATION_GUIDE.md) for upgrade instructions |
| 2024 May 21 |  support Espresso Sentence Embeddings  |
| 2024 Feb 7 |  support training with only positive pairs (Format C: query, positive)  |
| 2023 Dec 4 |  Release a universal English sentence embedding model: [WhereIsAI/UAE-Large-V1](https://huggingface.co/WhereIsAI/UAE-Large-V1)  |
| 2023 Nov 2 |  Release an English pretrained model: `SeanLee97/angle-llama-13b-nli` |
| 2023 Oct 28 |  Release two chinese pretrained models: `SeanLee97/angle-roberta-wwm-base-zhnli-v1` and `SeanLee97/angle-llama-7b-zhnli-v1`; Add chinese README.md |

# üìß Contact

If you have any questions or suggestions, please feel free to contact us via email: xmlee97@gmail.com

# ¬© License

This project is licensed under the MIT License.
For the pretrained models, please refer to the corresponding license of the models.
